{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3c7c49-d738-4046-b5ad-4904ce9e4ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42b6a96-fcf9-4dc2-ad9c-42ae0d715c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import BedrockChat\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "IS_DOCS_ALREADY_EMBEDDED = False\n",
    "\n",
    "RAW_DATA_DIR = \"../raw_data/company/20240305\"\n",
    "FILENAME = \"1709599445799.pdf\"\n",
    "\n",
    "VECTOR_STORE_DIR = \"../vector_store\"\n",
    "INDEX_NAME = \"manual\"\n",
    "\n",
    "file_path = os.path.join(RAW_DATA_DIR, FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf2ab95-1dec-4623-b193-4d85cee61e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "raw_docs = loader.load()\n",
    "proc_docs = text_splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ab8c11-113c-46d8-9427-55d091dcdb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 6 documents loaded is 1718 characters.\n",
      "After the split we have 15 documents more than the original 6.\n",
      "Average length among 15 documents (after split) is 731 characters.\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs]) // len(docs)\n",
    "\n",
    "avg_char_count_raw = avg_doc_length(raw_docs)\n",
    "avg_char_count_proc = avg_doc_length(proc_docs)\n",
    "\n",
    "print(\n",
    "    f\"Average length among {len(raw_docs)} documents loaded is {avg_char_count_raw} characters.\"\n",
    ")\n",
    "print(\n",
    "    f\"After the split we have {len(proc_docs)} documents more than the original {len(raw_docs)}.\"\n",
    ")\n",
    "print(\n",
    "    f\"Average length among {len(proc_docs)} documents (after split) is {avg_char_count_proc} characters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b24e83f-9649-46f9-bc43-78706e3cf56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 232 ms, sys: 52.6 ms, total: 285 ms\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = BedrockEmbeddings(\n",
    "    region_name=\"us-east-1\", model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "if IS_DOCS_ALREADY_EMBEDDED:\n",
    "    vector_store = FAISS.load_local(\n",
    "        os.path.join(VECTOR_STORE_DIR, INDEX_NAME), embeddings\n",
    "    )\n",
    "\n",
    "else:\n",
    "    vector_store = FAISS.from_documents(proc_docs, embeddings)\n",
    "    vector_store.save_local(os.path.join(VECTOR_STORE_DIR, INDEX_NAME))\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a64ce6a-b122-4dd7-b8fd-1db6494b3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = BedrockChat(\n",
    "    region_name=\"us-east-1\",\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\": 1024},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94a2ca3-eac4-49f7-bce6-b608cb8cd7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"Use the following pieces of context to answer the user's question. If you don't know the answer,\n",
    "just say that you don't know, don't try to make up an answer. Answer in Korean.\n",
    "----------------\n",
    "{context}\"\"\"\n",
    "\n",
    "human_template = \"{question}\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(human_template),\n",
    "]\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": ChatPromptTemplate.from_messages(messages)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95ed7b5-50a9-45d1-b096-30ab317c26d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youngmki/anaconda3/envs/wealth-whisper/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성생명의 목표주가는 125,000원으로 기존 대비 24.5% 상향 조정되었습니다. 목표주가 상향 조정 이유는 2024년부터 BPS(Book Price per Share, 주당순자산가치)를 적용하고 할인율을 조정했기 때문입니다. 이에 따라 삼성생명의 목표 PBR(주가순자산비율)은 0.45배가 적용되었습니다.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": \"삼성생명 목표 주가는 얼마야? 그 이유는?\"})\n",
    "print(result[\"result\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e35f5c4c-64b6-4dc0-b24c-5cf13101950a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성생명의 최근 주가수익률은 다음과 같습니다.\n",
      "\n",
      "1개월 수익률: 47.5%\n",
      "3개월 수익률: 39.6% \n",
      "6개월 수익률: 43.8%\n",
      "12개월 수익률: 42.3%\n",
      "\n",
      "상대수익률(KOSPI 대비)\n",
      "1개월: 39.6%\n",
      "3개월: 33.1%  \n",
      "6개월: 38.9%\n",
      "12개월: 29.9%\n",
      "\n",
      "최근 1년간 삼성생명 주가는 KOSPI 지수를 상회하는 높은 수익률을 기록했습니다. 특히 단기(1개월, 3개월, 6개월)에서 40% 내외의 높은 절대수익률을 보였습니다.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": \"삼성생명 최근 주가 수익률 알려줘\"})\n",
    "print(result[\"result\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17de9fc7-786b-473e-8e05-a0ecf1d46ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성생명의 상품별 CSM 배수 최근 추세에 대해서는 정확한 정보를 갖고 있지 않습니다. 하지만 제공된 자료에서 일부 내용을 확인할 수 있습니다.\n",
      "\n",
      "그림 3에서 2022년 4분기 기준 상품별 CSM 배수를 보여주고 있습니다. \n",
      "- 사망보장성 상품의 CSM 배수는 약 0.8배 수준\n",
      "- 건강보장성 상품의 CSM 배수는 약 1.2배 수준 \n",
      "- 연금/저축성 상품의 CSM 배수는 약 0.4배 수준\n",
      "\n",
      "이를 통해 건강보장성 상품의 CSM 배수가 가장 높고, 연금/저축성 상품이 가장 낮은 것으로 나타납니다. 하지만 시점별 추이는 알려지지 않아 최근 추세를 파악하기는 어렵습니다. 상품별 CSM 배수의 변동 요인과 향후 전망 등에 대해서는 추가 정보가 필요할 것 같습니다.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": \"삼성생명 상품별 CSM 배수 최근 추세 알려줘\"})\n",
    "print(result[\"result\"].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
